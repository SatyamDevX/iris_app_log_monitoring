# ğŸŒ¸ Iris Classifier with GKE, FastAPI, Logging & Telemetry

This project demonstrates a full-stack MLOps deployment of an Iris classifier using **FastAPI**, **Docker**, and **Google Kubernetes Engine (GKE)** with built-in **structured logging**, **Google Cloud Trace**, **health probes**, and **autoscaling**.

---

## ğŸš€ Features

- ğŸ” Real-time Iris flower classification (`/predict` endpoint)
- ğŸ“ˆ Autoscaling using Kubernetes HPA (based on CPU)
- ğŸ”§ Structured logs to Google Cloud Logging
- ğŸ“Š Tracing with OpenTelemetry â†’ Cloud Trace
- âœ… Liveness & readiness probes
- ğŸ‹ Dockerized & deployed on GKE
- âš™ï¸ Workload Identity-based GCP access

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ demo_log.py              # FastAPI app with ML logic, tracing & logging
â”œâ”€â”€ iris-model.joblib        # Trained iris model
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile               # Container setup
â”œâ”€â”€ deployment.yaml          # Kubernetes Deployment (includes probes + SA)
â”œâ”€â”€ service.yaml             # Kubernetes Service (LoadBalancer)
â”œâ”€â”€ hpa.yaml                 # Horizontal Pod Autoscaler config
â”œâ”€â”€ post.lua                 # wrk load test script
```

---

## âš™ï¸ Prerequisites

- âœ… GCP Project with Billing Enabled
- âœ… Artifact Registry created (e.g., `my-repo`)
- âœ… `gcloud` CLI installed and authenticated
- âœ… Kubernetes cluster created with Workload Identity

---

## ğŸ“¦ Build and Push Docker Image

```bash
docker build -t demo_log .
docker tag demo_log us-central1-docker.pkg.dev/YOUR_PROJECT_ID/my-repo/demo_log:latest
docker push us-central1-docker.pkg.dev/YOUR_PROJECT_ID/my-repo/demo_log:latest
```

---

## â˜¸ï¸ Deploy on GKE

```bash
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
kubectl apply -f hpa.yaml
```

Then get external IP:

```bash
kubectl get service demo-log-ml-service
```

Test it:

```bash
curl -X POST http://<EXTERNAL-IP>:80/predict \
  -H "Content-Type: application/json" \
  -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
```

---

## ğŸ“Š Load Testing

Install `wrk`:
```bash
sudo apt install wrk
```

Create `post.lua`:

```lua
wrk.method = "POST"
wrk.body   = '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
wrk.headers["Content-Type"] = "application/json"
```

Run load test:

```bash
wrk -t4 -c100 -d30s --latency -s post.lua http://<EXTERNAL-IP>:80/predict
```

Monitor autoscaler:

```bash
kubectl get hpa
kubectl get pods -w
```

---

## ğŸ” Observe Logs & Traces

### âœ… Logs
[Cloud Logging Dashboard](https://console.cloud.google.com/logs/query)

- Check for structured logs from `demo-log-ml-service`
- Filter by `resource.type="k8s_container"` and severity `INFO`

### âœ… Traces
[Cloud Trace Dashboard](https://console.cloud.google.com/traces/list)

- View latency and trace IDs for `/predict` endpoint

---

## âœ… Health Checks

- `/live_check` â†’ for liveness probe
- `/ready_check` â†’ for readiness (after model load delay)

---

## ğŸ§  Model Input Format

```json
{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}
```

---

## ğŸ™Œ Author

**Satyam Srivastava** â€” _Deployed via GKE with â¤ï¸_

---

## ğŸ“Œ License

This project is for educational/demo purposes only.
